{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from https://www.kaggle.com/code/cdeotte/prot-bert-finetune-lb-0-30cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR=\"/home/lilian/TCR_Graphs/encodingesm2_output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esm2 detected\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# CFG\n",
    "# ====================================================\n",
    "class CFG:\n",
    "    wandb=False\n",
    "    competition='FB3'\n",
    "    _wandb_kernel='nakama'\n",
    "    debug=False\n",
    "    apex=True\n",
    "    print_freq=20\n",
    "    num_workers=0\n",
    "    # Suggest: Rostlab/prot_bert or ESMs at https://huggingface.co/models?sort=downloads&search=esm\n",
    "    # ESM examples: facebook/esm2_t33_650M_UR50D, facebook/esm1v_t33_650M_UR90S_1, facebook/esm1b_t33_650M_UR50S, etc \n",
    "    model=\"facebook/esm2_t33_650M_UR50D\" \n",
    "    gradient_checkpointing=False\n",
    "    scheduler='constant' # ['linear', 'cosine', 'constant']\n",
    "    batch_scheduler=True\n",
    "    num_warmup_steps=0\n",
    "    \n",
    "    # LEARNING RATE\n",
    "    # Suggested for batchsize=8: Prot_bert = 5e-6, ESM2 = 5e-5\n",
    "    epochs=1\n",
    "    num_cycles=1.0 #only affects cosine schedule\n",
    "    encoder_lr=5e-5\n",
    "    decoder_lr=5e-5\n",
    "    batch_size=8\n",
    "    \n",
    "    # MODEL INFO - PROT_BERT\n",
    "    total_blocks = 30 \n",
    "    initial_layers = 5 \n",
    "    layers_per_block = 16 \n",
    "    # MODEL INFO - FACEBOOK ESM2\n",
    "    if 'esm2' in model:\n",
    "        total_blocks = int(model.split('_')[1][1:])\n",
    "        initial_layers = 2 \n",
    "        layers_per_block = 16 \n",
    "        print('esm2 detected')\n",
    "    # MODEL INFO - FACEBOOK ESM1B\n",
    "    elif 'esm1b' in model:\n",
    "        total_blocks = int(model.split('_')[1][1:])\n",
    "        initial_layers = 4 \n",
    "        layers_per_block = 16 \n",
    "        print('esm1b detected')\n",
    "    # MODEL INFO - FACEBOOK ESM1V\n",
    "    elif 'esm1v' in model:\n",
    "        total_blocks = int(model.split('_')[1][1:])\n",
    "        initial_layers = 2 \n",
    "        layers_per_block = 16 \n",
    "        print('esm1v detected')\n",
    "    else:\n",
    "        print('prot_bert detected')\n",
    "        \n",
    "    # FREEZE\n",
    "    # Suggested: Prot_bert -8, ESM2 -3\n",
    "    num_freeze_blocks = total_blocks - 3\n",
    "    \n",
    "    # FOR NO FREEZE USE\n",
    "    #num_freeze_blocks = 0\n",
    "    \n",
    "    min_lr=1e-6\n",
    "    eps=1e-6\n",
    "    betas=(0.9, 0.999)\n",
    "    max_len=512\n",
    "    weight_decay=0.01\n",
    "    gradient_accumulation_steps=1\n",
    "    max_grad_norm=1000\n",
    "    target_cols=['target']\n",
    "    seed=42\n",
    "    n_fold=5\n",
    "    trn_fold=[0,1,2,3,4]\n",
    "    train=True\n",
    "    pca_dim = 64\n",
    "    \n",
    "if CFG.debug:\n",
    "    CFG.epochs = 2\n",
    "    CFG.trn_fold = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# wandb\n",
    "# ====================================================\n",
    "if CFG.wandb:\n",
    "    \n",
    "    import wandb\n",
    "\n",
    "    try:\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "        user_secrets = UserSecretsClient()\n",
    "        secret_value_0 = user_secrets.get_secret(\"wandb_api\")\n",
    "        wandb.login(key=secret_value_0)\n",
    "        anony = None\n",
    "    except:\n",
    "        anony = \"must\"\n",
    "        print('If you want to use your W&B account, go to Add-ons -> Secrets and provide your W&B access token. Use the Label name as wandb_api. \\nGet your W&B access token from here: https://wandb.ai/authorize')\n",
    "\n",
    "\n",
    "    def class2dict(f):\n",
    "        return dict((name, getattr(f, name)) for name in dir(f) if not name.startswith('__'))\n",
    "\n",
    "    run = wandb.init(project='FB3-Public', \n",
    "                     name=CFG.model,\n",
    "                     config=class2dict(CFG),\n",
    "                     group=CFG.model,\n",
    "                     job_type=\"train\",\n",
    "                     anonymous=anony)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Obtaining dependency information for transformers from https://files.pythonhosted.org/packages/1a/d1/3bba59606141ae808017f6fde91453882f931957f125009417b87a281067/transformers-4.34.0-py3-none-any.whl.metadata\n",
      "  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from transformers) (3.9.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/aa/f3/3fc97336a0e90516901befd4f500f08d691034d387406fdbde85bea827cc/huggingface_hub-0.17.3-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.17.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Obtaining dependency information for regex!=2019.12.17 from https://files.pythonhosted.org/packages/f2/b8/b1ec82fce93064a73ba67f2bb158ec9cac4a0e8f0b6942268ec963947329/regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from transformers) (2.28.1)\n",
      "Collecting tokenizers<0.15,>=0.14 (from transformers)\n",
      "  Obtaining dependency information for tokenizers<0.15,>=0.14 from https://files.pythonhosted.org/packages/45/19/bfdbe9504896938cce00cab78b2847d2c1fbda5201704d7bf691699b70b9/tokenizers-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading tokenizers-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.3.1 (from transformers)\n",
      "  Obtaining dependency information for safetensors>=0.3.1 from https://files.pythonhosted.org/packages/9c/ee/de72fa290bef49b22aacdf112e70f40d3b1beb526dd65e94e0f540d192f0/safetensors-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
      "  Downloading safetensors-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from transformers) (4.66.1)\n",
      "Collecting fsspec (from huggingface-hub<1.0,>=0.16.4->transformers)\n",
      "  Obtaining dependency information for fsspec from https://files.pythonhosted.org/packages/fe/d3/e1aa96437d944fbb9cc95d0316e25583886e9cd9e6adc07baad943524eda/fsspec-2023.9.2-py3-none-any.whl.metadata\n",
      "  Downloading fsspec-2023.9.2-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
      "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n",
      "  Obtaining dependency information for huggingface-hub<1.0,>=0.16.4 from https://files.pythonhosted.org/packages/7f/c4/adcbe9a696c135578cabcbdd7331332daad4d49b7c43688bc2d36b3a47d2/huggingface_hub-0.16.4-py3-none-any.whl.metadata\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from requests->transformers) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/lilian/miniconda3/envs/py311-pyg-cuda118/lib/python3.11/site-packages (from requests->transformers) (2022.12.7)\n",
      "Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.10.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (785 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m785.1/785.1 kB\u001b[0m \u001b[31m92.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.14.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m87.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m90.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2023.9.2-py3-none-any.whl (173 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed fsspec-2023.9.2 huggingface-hub-0.16.4 regex-2023.10.3 safetensors-0.3.3 tokenizers-0.14.0 transformers-4.34.0\n",
      "tokenizers.__version__: 0.14.0\n",
      "transformers.__version__: 4.34.0\n",
      "env: TOKENIZERS_PARALLELISM=true\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Library\n",
    "# ====================================================\n",
    "import os\n",
    "import gc\n",
    "import re\n",
    "import ast\n",
    "import sys\n",
    "import copy\n",
    "import json\n",
    "import time\n",
    "import math\n",
    "import string\n",
    "import pickle\n",
    "import random\n",
    "import joblib\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, GroupKFold, KFold\n",
    "\n",
    "#os.system('pip install iterative-stratification==0.1.7')\n",
    "#from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam, SGD, AdamW\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#os.system('pip uninstall -y transformers')\n",
    "#os.system('pip uninstall -y tokenizers')\n",
    "#os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels transformers')\n",
    "#os.system('python -m pip install --no-index --find-links=../input/fb3-pip-wheels tokenizers')\n",
    "os.system('pip install transformers --upgrade')\n",
    "#os.system('pip install tokenizers --upgrade')\n",
    "\n",
    "import tokenizers\n",
    "import transformers\n",
    "print(f\"tokenizers.__version__: {tokenizers.__version__}\")\n",
    "print(f\"transformers.__version__: {transformers.__version__}\")\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "from transformers import get_constant_schedule_with_warmup\n",
    "%env TOKENIZERS_PARALLELISM=true\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# Utils\n",
    "# ====================================================\n",
    "def MCRMSE(y_trues, y_preds):\n",
    "    scores = []\n",
    "    idxes = y_trues.shape[1]\n",
    "    for i in range(idxes):\n",
    "        y_true = y_trues[:,i]\n",
    "        y_pred = y_preds[:,i]\n",
    "        score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n",
    "        scores.append(score)\n",
    "    mcrmse_score = np.mean(scores)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_score(y_trues, y_preds):\n",
    "    mcrmse_score, scores = MCRMSE(y_trues, y_preds)\n",
    "    return mcrmse_score, scores\n",
    "\n",
    "\n",
    "def get_logger(filename=OUTPUT_DIR+'train'):\n",
    "    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n",
    "    logger = getLogger(__name__)\n",
    "    logger.setLevel(INFO)\n",
    "    handler1 = StreamHandler()\n",
    "    handler1.setFormatter(Formatter(\"%(message)s\"))\n",
    "    handler2 = FileHandler(filename=f\"{filename}.log\")\n",
    "    handler2.setFormatter(Formatter(\"%(message)s\"))\n",
    "    logger.addHandler(handler1)\n",
    "    logger.addHandler(handler2)\n",
    "    return logger\n",
    "\n",
    "LOGGER = get_logger()\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    \n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold\n",
    "# ====================================================\n",
    "# CV split\n",
    "# ====================================================\n",
    "Fold = GroupKFold(n_splits=CFG.n_fold) \n",
    "for n, (train_index, val_index) in enumerate(Fold.split(train, train[CFG.target_cols], train.PDB)):\n",
    "    train.loc[val_index, 'fold'] = int(n)\n",
    "train['fold'] = train['fold'].astype(int)\n",
    "display(train.groupby('fold').size())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py311-pyg-cuda118",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
